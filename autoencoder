# -*- coding: utf-8 -*-
"""
Autoencoder Hyperparameter Optimization for Molecular Fingerprints
=================================================================

Author: vicre
Created: Wed Oct 9 23:09:15 2024

Description:
------------
This script performs hyperparameter optimization of an Autoencoder neural network
using Optuna. The Autoencoder is used to compress molecular fingerprints into a 
32-dimensional latent representation. Once the best hyperparameters are found, 
the final model is trained for effective data compression.

Main Features:
--------------
- **Optuna** for hyperparameter optimization
- **Dropout** and **L2 regularization** for reducing overfitting
- **Residual connections** and **Batch Normalization** to improve gradient flow
- **Adam optimizer** for gradient descent
- **Reproducibility** ensured by setting random seeds
"""

import optuna
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import random

# ----------------------------- #
# 1. SEEDING FOR REPRODUCIBILITY
# ----------------------------- #
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True # Ensures reproducible results
    torch.backends.cudnn.benchmark = False    # Disables auto-optimizations that vary across runs

# ----------------------------- #
# 2. DATA LOADING & PREPROCESSING
# ----------------------------- #
'''
Datasets:
---------
FA dataset columns:
    FA: ID, Nickname, Reference, PCE_max(%), PCE_ave(%), Voc(V), Jsc(mA/cm2), FF,
        Mw, Mn, PDI, Monomer, HOMO/LUMO levels, Eg, p(CDK/RDKit/Morgan FPs), 
        Acceptor, n(CDK/RDKit/Morgan FPs)

NFA dataset columns:
    NFA: ID, Ref, PCE_max(%), PCE_ave(%), Jsc(mA/cm2), FF, Voc(V),
         HOMO/LUMO levels, Eg, SMILES, Mw, Mn, PDI, 
         n(CDK/RDKit/Morgan FPs), p(CDK/RDKit/Morgan FPs)
'''

def is_binary(fp):
    """Check whether a given fingerprint string consists only of 0s and 1s."""
    return isinstance(fp, str) and all(char in '01' for char in fp)

# --- Load FA dataset ---
df1 = pd.read_csv(r'D:\Ciências\Unb\Pibic OPV\Computation and Scripts\Computation and Scripts\Dataset\FA_Polymer\final_database_FA_with_acceptor_fp.csv', delimiter=',').drop(['ID', 'Nickname', 'Ref'], axis=1)
# Keep only rows where all fingerprints are valid binary strings
df1 = df1[df1[['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)']].apply(lambda x: x.map(is_binary)).all(axis=1)]
# Concatenate donor & acceptor fingerprints into combined features
df1['CDK_fp']    = df1['n(CDK Fingerprint)'] + df1['p(CDK Fingerprint)']
df1['RDKit_fp']  = df1['n(RDKit Fingerprint)'] + df1['p(RDKit Fingerprint)']
df1['Morgan_fp'] = df1['n(Morgan Fingerprint)'] + df1['p(Morgan Fingerprint)']
# Select only relevant columns
df1 = df1[['CDK_fp', 'RDKit_fp', 'Morgan_fp', 'PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']].reset_index(drop=True)

# --- Load NFA dataset ---
df2 = pd.read_csv(r'D:\Ciências\Unb\Pibic OPV\Computation and Scripts\Computation and Scripts\Dataset\NFA_Polymer\final_database_NFA.csv', delimiter=',').drop(['ID', 'Ref', 'n(SMILES)', 'p(SMILES)'], axis=1)
df2 = df2[df2[['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)']].apply(lambda x: x.map(is_binary)).all(axis=1)]
df2['CDK_fp']    = df2['n(CDK Fingerprint)'] + df2['p(CDK Fingerprint)']
df2['RDKit_fp']  = df2['n(RDKit Fingerprint)'] + df2['p(RDKit Fingerprint)']
df2['Morgan_fp'] = df2['n(Morgan Fingerprint)'] + df2['p(Morgan Fingerprint)']
df2 = df2[['CDK_fp', 'RDKit_fp', 'Morgan_fp', 'PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']].reset_index(drop=True)

# Combine FA and NFA datasets into one dataframe
df = pd.concat([df1, df2], ignore_index=True)
df = df.sample(frac=1).reset_index(drop=True)

# Extract Morgan fingerprints as the primary feature set
X = df['Morgan_fp']

def convert_to_binary_vector(fp_str):
    """Convert a binary string fingerprint into a list of integers (0/1)."""
    if all(bit in '01' for bit in fp_str):
        return [int(bit) for bit in fp_str]
    else:
        raise ValueError(f"Invalid fingerprint format: {fp_str}")

# Convert all fingerprints to numeric binary vectors
try:
    binary_vectors = np.array([convert_to_binary_vector(fp) for fp in X.values])
except ValueError as e:
    print(e)

# ----------------------------- #
# 3. TRAIN/VALIDATION/TEST SPLIT
# ----------------------------- #
# # Split dataset into training+validation and test sets (80/20 split)
X_train_val, X_test = train_test_split(binary_vectors, test_size=0.2, random_state=42)

# Further split training+validation set into training and validation (80/20 split)
X_train, X_val = train_test_split(X_train_val, test_size=0.2, random_state=42)

# ----------------------------- #
# 4. TENSOR CONVERSION & DEVICE SETUP
# ----------------------------- #
# Select GPU if available, else fallback to CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Convert numpy arrays to PyTorch tensors and move them to the chosen device
X_train_val_tensor = torch.tensor(X_train_val, dtype=torch.float32).to(device)
X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)

print(f"Training on: {device}")

# -----------------------------------------------------------
# 5. AUTOENCODER DEFINITION & HYPERPARAMETER OPTIMIZATION (OPTUNA)
# -----------------------------------------------------------
# Define the autoencoder with dropout as a hyperparameter
class FingerprintAutoencoder(nn.Module):
    def __init__(self, input_dim, embedding_dim, dropout_rate):
        super(FingerprintAutoencoder, self).__init__()
        # Encoder layers
        self.encoder_fc1 = nn.Linear(input_dim, 512)
        self.encoder_bn1 = nn.BatchNorm1d(512)
        self.encoder_dropout1 = nn.Dropout(dropout_rate)
        
        self.encoder_fc2 = nn.Linear(512, 256)
        self.encoder_bn2 = nn.BatchNorm1d(256)
        self.encoder_dropout2 = nn.Dropout(dropout_rate)
        
        self.encoder_fc3 = nn.Linear(256, 128)
        self.encoder_bn3 = nn.BatchNorm1d(128)
        self.encoder_dropout3 = nn.Dropout(dropout_rate)
        
        self.encoder_fc4 = nn.Linear(128, embedding_dim)
        
        # Decoder layers
        self.decoder_fc1 = nn.Linear(embedding_dim, 128)
        self.decoder_bn1 = nn.BatchNorm1d(128)
        self.decoder_dropout1 = nn.Dropout(dropout_rate)
        
        self.decoder_fc2 = nn.Linear(128, 256)
        self.decoder_bn2 = nn.BatchNorm1d(256)
        self.decoder_dropout2 = nn.Dropout(dropout_rate)
        
        self.decoder_fc3 = nn.Linear(256, 512)
        self.decoder_bn3 = nn.BatchNorm1d(512)
        self.decoder_dropout3 = nn.Dropout(dropout_rate)
        
        self.decoder_fc4 = nn.Linear(512, input_dim)

    def encode(self, x):
        residual = self.encoder_fc1(x)
        x = torch.relu(self.encoder_bn1(self.encoder_fc1(x)))
        x = self.encoder_dropout1(x)
        x = x + residual

        residual = self.encoder_fc2(x)
        x = torch.relu(self.encoder_bn2(self.encoder_fc2(x)))
        x = self.encoder_dropout2(x)
        x = x + residual

        residual = self.encoder_fc3(x)
        x = torch.relu(self.encoder_bn3(self.encoder_fc3(x)))
        x = self.encoder_dropout3(x)
        x = x + residual

        encoded = torch.relu(self.encoder_fc4(x))
        return encoded

    def decode(self, encoded):
        residual = self.decoder_fc1(encoded)
        x = torch.relu(self.decoder_bn1(self.decoder_fc1(encoded)))
        x = self.decoder_dropout1(x)
        x = x + residual

        residual = self.decoder_fc2(x)
        x = torch.relu(self.decoder_bn2(self.decoder_fc2(x)))
        x = self.decoder_dropout2(x)
        x = x + residual

        residual = self.decoder_fc3(x)
        x = torch.relu(self.decoder_bn3(self.decoder_fc3(x)))
        x = self.decoder_dropout3(x)
        x = x + residual

        decoded = torch.sigmoid(self.decoder_fc4(x))
        return decoded

    def forward(self, x):
        encoded = self.encode(x)
        decoded = self.decode(encoded)
        return encoded, decoded

# ----------------------------- #
# 6. HYPERPARAMETER OPTIMIZATION
# ----------------------------- #
embedding_dim = 32  # Latent space dimension
epochs = 100        # Training epochs per trial

# Optuna objective function
def objective(trial):
    """
    Optuna objective function for hyperparameter optimization.

    Returns:
    --------
    Validation loss (float) to be minimized by Optuna.
    """
    # Hyperparameters to tune
    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.9)  # Dropout rate
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)  # Learning rate
    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)  # Weight decay

    # Model
    model = FingerprintAutoencoder(input_dim=X_train_tensor.shape[1], embedding_dim=embedding_dim, dropout_rate=dropout_rate).to(device)
    
    # Optimizer and loss function
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)  # Include weight_decay
    criterion = nn.BCELoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        optimizer.zero_grad()
        _, reconstructed = model(X_train_tensor)
        loss = criterion(reconstructed, X_train_tensor)
        loss.backward()
        optimizer.step()

    # Evaluate the model on validation set
    model.eval()
    with torch.no_grad():
        _, reconstructed = model(X_val_tensor)
        val_loss = criterion(reconstructed, X_val_tensor)

    return val_loss.item()

# Create Optuna study
study = optuna.create_study(direction='minimize')
# Run the optimization
study.optimize(objective, n_trials=200)

# Display the best hyperparameters
print('Best trial:')
trial = study.best_trial
print(f'Loss: {trial.value}')
best_params = study.best_trial.params
print("Best hyperparameters found by Optuna:")
print(best_params)

# ----------------------------- #
# 7. FINAL TRAINING WITH BEST PARAMS
# ----------------------------- #
# Train the final model using the best hyperparameters on full train+val set
best_dropout_rate = best_params['dropout_rate']
best_learning_rate = best_params['learning_rate']
best_weight_decay = best_params['weight_decay']

final_model = FingerprintAutoencoder(input_dim=X_train_tensor.shape[1], embedding_dim=embedding_dim, dropout_rate=best_dropout_rate).to(device)

# Optimizer and loss function with the best hyperparameters
final_optimizer = optim.Adam(final_model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay)
criterion = nn.BCELoss()

# Final training loop
final_model.train()
for epoch in range(epochs):
    final_optimizer.zero_grad()
    _, reconstructed = final_model(X_train_val_tensor)
    loss = criterion(reconstructed, X_train_val_tensor)
    loss.backward()
    final_optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

# Evaluation on the test set
final_model.eval()
with torch.no_grad():
    _, test_reconstructed = final_model(X_test_tensor)
    test_loss = criterion(test_reconstructed, X_test_tensor)

print(f'Test Loss: {test_loss.item():.4f}')

# -----------------------------------------------------------
# RESULTS SAVING, DATA COMPRESSION & MODEL SERIALIZATION
# -----------------------------------------------------------

# ----------------------------- #
# 1. SAVE RESULTS TO FILE
# ----------------------------- #

# Path to the output file
output_file = 'autoencoder_results.txt'

# Write the results to the file
with open(output_file, 'w') as f:
    f.write('Best Trial:\n')
    f.write(f'Validation Loss (Optuna): {trial.value:.6f}\n\n')
    
    f.write('Best Hyperparameters:\n')
    for key, value in best_params.items():
        f.write(f'{key}: {value}\n')
    
    f.write(f'\nTest Loss: {test_loss.item():.6f}\n')

print(f"\nResults saved to '{output_file}'")

# ----------------------------- #
# 2. COMPRESS DATA USING TRAINED ENCODER
# ----------------------------- #

# Convert original Morgan fingerprints into a tensor for encoding
X_tensor = torch.tensor(np.array([convert_to_binary_vector(fp) for fp in X.values]), dtype=torch.float32).to(device)

# Pass data through the trained encoder to obtain latent representations
with torch.no_grad():
    encoded_data = final_model.encode(X_tensor)  # Only capture a single output

# Convert encoded data to numpy array and create DataFrame
encoded_df = pd.DataFrame(encoded_data.cpu().numpy())

# Combine the original fingerprint strings with their encoded representation
# (keeps track of which compressed vector corresponds to which molecule)
combined_df = pd.concat([X.to_frame(), encoded_df], axis=1)  # Ensure X is a DataFrame

# Save the combined dataset to a CSV file
combined_df.to_csv('combined_data.csv', index=False)

# ----------------------------- #
# 3. SAVE TRAINED MODEL
# ----------------------------- #

# Save the full autoencoder (entire architecture + trained weights)
torch.save(final_model.state_dict(), 'autoencoder_model.pth')

# Additionally, save encoder and decoder weights separately
encoder_state_dict = {
    'encoder_fc1': final_model.encoder_fc1.state_dict(),
    'encoder_bn1': final_model.encoder_bn1.state_dict(),
    'encoder_fc2': final_model.encoder_fc2.state_dict(),
    'encoder_bn2': final_model.encoder_bn2.state_dict(),
    'encoder_fc3': final_model.encoder_fc3.state_dict(),
    'encoder_bn3': final_model.encoder_bn3.state_dict(),
    'encoder_fc4': final_model.encoder_fc4.state_dict(),
}

decoder_state_dict = {
    'decoder_fc1': final_model.decoder_fc1.state_dict(),
    'decoder_bn1': final_model.decoder_bn1.state_dict(),
    'decoder_fc2': final_model.decoder_fc2.state_dict(),
    'decoder_bn2': final_model.decoder_bn2.state_dict(),
    'decoder_fc3': final_model.decoder_fc3.state_dict(),
    'decoder_bn3': final_model.decoder_bn3.state_dict(),
    'decoder_fc4': final_model.decoder_fc4.state_dict(),
}

torch.save(encoder_state_dict, 'encoder.pth')
torch.save(decoder_state_dict, 'decoder.pth')

#### Loading ####
'''
model = FingerprintAutoencoder(input_dim, embedding_dim, dropout_rate)

model.load_state_dict(torch.load('autoencoder_model.pth'))

# Load the encoder and decoder weights back into the model
model.encoder_fc1.load_state_dict(torch.load('encoder.pth')['encoder_fc1'])
model.encoder_bn1.load_state_dict(torch.load('encoder.pth')['encoder_bn1'])
model.encoder_fc2.load_state_dict(torch.load('encoder.pth')['encoder_fc2'])
model.encoder_bn2.load_state_dict(torch.load('encoder.pth')['encoder_bn2'])
model.encoder_fc3.load_state_dict(torch.load('encoder.pth')['encoder_fc3'])
model.encoder_bn3.load_state_dict(torch.load('encoder.pth')['encoder_bn3'])
model.encoder_fc4.load_state_dict(torch.load('encoder.pth')['encoder_fc4'])

model.decoder_fc1.load_state_dict(torch.load('decoder.pth')['decoder_fc1'])
model.decoder_bn1.load_state_dict(torch.load('decoder.pth')['decoder_bn1'])
model.decoder_fc2.load_state_dict(torch.load('decoder.pth')['decoder_fc2'])
model.decoder_bn2.load_state_dict(torch.load('decoder.pth')['decoder_bn2'])
model.decoder_fc3.load_state_dict(torch.load('decoder.pth')['decoder_fc3'])
model.decoder_bn3.load_state_dict(torch.load('decoder.pth')['decoder_bn3'])
model.decoder_fc4.load_state_dict(torch.load('decoder.pth')['decoder_fc4'])
'''


'''
# Load the saved encoder model
encoder = FingerprintAutoencoder(input_dim=X_train_tensor.shape[1], embedding_dim=embedding_dim, dropout_rate=best_dropout_rate).to(device)
encoder.load_state_dict(torch.load('encoder.pth'))

# Convert the 'Morgan_fp' column to binary vectors
binary_vectors = np.array([convert_to_binary_vector(fp) for fp in X.values])

# Convert to PyTorch tensor and move to GPU
X_tensor = torch.tensor(binary_vectors, dtype=torch.float32).to(device)

# Get the encoded representation using the loaded encoder
with torch.no_grad():
    encoded_data, _ = encoder(X_tensor)

# Convert encoded data to numpy array and create DataFrame
encoded_df = pd.DataFrame(encoded_data.cpu().numpy())

# Concatenate the encoded DataFrame with the original DataFrame X
combined_df = pd.concat([X.to_frame(), encoded_df], axis=1)

# Save the combined DataFrame to a CSV file
combined_df.to_csv('combined_data.csv', index=False)
'''
