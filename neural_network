# -*- coding: utf-8 -*-
"""
Created on Sat Oct  5 13:12:07 2024
@author: vicre

Neural Network Hyperparameter Optimization for Organic Solar Cell (OSC) Device Prediction

This script:
- Loads and preprocesses OSC datasets (FA-Polymer and NFA-Polymer)
- Optimizes hyperparameters of a Neural Network using Optuna
- Implements a residual network with batch normalization, dropout, and L2 regularization
- Uses k-fold cross-validation during optimization
- Trains a final model with the best hyperparameters
- Evaluates the model on a hold-out test set and computes performance metrics
"""

import torch
import torch.nn as nn
import torch.optim as optim
import optuna
from sklearn.model_selection import KFold
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from scipy.stats import pearsonr
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ------------------------------------------------------------------------------
# DEVICE CONFIGURATION
# ------------------------------------------------------------------------------

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

# ------------------------------------------------------------------------------
# DATA LOADING & PREPROCESSING
# ------------------------------------------------------------------------------

# Load FA-Polymer dataset, clean columns, convert numeric fields, and drop missing values
df1 = pd.read_csv('final_database_FA.csv', delimiter=',').drop(['ID', 'Nickname', 'Ref'], axis=1)
df1 = df1.rename(columns={'PDI(=Mw/Mn)': 'PDI', 'Monomer(g/m)':'M(g/mol)'})
df1[['Mn(kg/mol)', 'PDI']] = df1[['Mn(kg/mol)', 'PDI']].replace('-', np.nan)
df1[['Mn(kg/mol)', 'PDI']] = df1[['Mn(kg/mol)', 'PDI']].apply(pd.to_numeric, errors='coerce')
df1= df1.drop(['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)'], axis=1)
df1 = df1.dropna()

# Load NFA-Polymer dataset and apply similar cleaning
df2 = pd.read_csv('final_database_NFA.csv', delimiter=',').drop(['ID', 'Ref', 'n(SMILES)', 'p(SMILES)'], axis=1)
df2 = df2.drop(['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)'], axis=1)
df2 = df2.dropna()

# Merge datasets and shuffle rows
df = pd.concat([df1, df2], ignore_index=True)
df = df.sample(frac=1).reset_index(drop=True)

X = df[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI']]
y = df[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']]
X_1 = df1[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI']]
y_1 = df1[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']]
X_2 = df2[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI']]
y_2 = df2[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler_X = StandardScaler()
X_train = scaler_X.fit_transform(X_train)
X_test = scaler_X.transform(X_test)

scaler_y = StandardScaler()
y_train = scaler_y.fit_transform(y_train)
y_test = scaler_y.transform(y_test)

# Convert data to PyTorch tensors and move to GPU (if available)
X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)

# ------------------------------------------------------------------------------
# NEURAL NETWORK MODEL DEFINITION
# ------------------------------------------------------------------------------
class ResidualNet(nn.Module):
    """
    Fully-connected Residual Network with:
    - Batch Normalization
    - Dropout for regularization
    - Residual connection between input and third hidden layer
    - 10 input features, 4 output targets
    """
    def __init__(self, dropout_rate):
        super(ResidualNet, self).__init__()
        self.fc1 = nn.Linear(10, 64)  # 10 input features, 64 hidden units
        self.bn1 = nn.BatchNorm1d(64)  # Batch Normalization
        self.fc2 = nn.Linear(64, 128)
        self.bn2 = nn.BatchNorm1d(128)
        self.fc3 = nn.Linear(128, 64)
        self.bn3 = nn.BatchNorm1d(64)
        self.fc4 = nn.Linear(64, 4)  # 4 output features

        self.dropout_1 = nn.Dropout(dropout_rate)  # Dropout for regularization
        self.dropout_2 = nn.Dropout(dropout_rate)
        self.identity_fc = nn.Linear(10, 64)

    def forward(self, x):
        identity = self.identity_fc(x)
        x = torch.relu(self.bn1(self.fc1(x)))
        x = self.dropout_1(x)
        x = torch.relu(self.bn2(self.fc2(x)))
        x = self.dropout_2(x)
        x = self.bn3(self.fc3(x))
        x += identity  # Residual/skip connection
        x = torch.relu(x)
        x = self.fc4(x)
        return x

epochs = 100

# ------------------------------------------------------------------------------
# TRAINING FUNCTION WITH CROSS-VALIDATION
# ------------------------------------------------------------------------------

def train_and_evaluate(lr, weight_decay, dropout_rate):
    """
    Train and evaluate the model using k-fold cross-validation.
    Returns the mean validation loss across folds.
    """
    kf = KFold(n_splits=3)
    criterion = nn.MSELoss()  # Mean Squared Error for regression
    batch_size = 32
    total_val_loss = 0

    for train_idx, val_idx in kf.split(X_train_tensor):
        model = ResidualNet(dropout_rate=dropout_rate).to(device)
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

        # Split data for cross-validation
        train_dataset = TensorDataset(X_train_tensor[train_idx], y_train_tensor[train_idx])
        val_dataset = TensorDataset(X_train_tensor[val_idx], y_train_tensor[val_idx])
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        for epoch in range(epochs):
            model.train()
            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()

        # Validation phase
        model.eval()
        with torch.no_grad():
            val_loss = 0
            for val_X, val_y in val_loader:
                val_X, val_y = val_X.to(device), val_y.to(device)
                predictions = model(val_X)
                val_loss += criterion(predictions, val_y).item()
            total_val_loss += val_loss / len(val_loader)

    return total_val_loss / kf.get_n_splits()

# ------------------------------------------------------------------------------
# OPTUNA HYPERPARAMETER OPTIMIZATION
# ------------------------------------------------------------------------------

# Optuna objective function
def objective(trial):
    lr = trial.suggest_float('lr', 1e-5, 1, log=True)
    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-1, log=True)
    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.7)
    return train_and_evaluate(lr, weight_decay, dropout_rate)

# Optimize hyperparameters with Optuna
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=200)

# Best hyperparameters
print(f'Best hyperparameters: {study.best_params}')
print(f'Best validation loss: {study.best_value}')

# ------------------------------------------------------------------------------
# FINAL TRAINING WITH BEST HYPERPARAMETERS
# ------------------------------------------------------------------------------

# Train on entire training set with the best hyperparameters and save model
best_model = ResidualNet(dropout_rate=study.best_params['dropout_rate']).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(best_model.parameters(), lr=study.best_params['lr'], weight_decay=study.best_params['weight_decay'])

# Full training
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)

for epoch in range(epochs):
    best_model.train()
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
        optimizer.zero_grad()
        outputs = best_model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()

# ------------------------------------------------------------------------------
# MODEL EVALUATION ON TEST SET
# ------------------------------------------------------------------------------

best_model.eval()
with torch.no_grad():
    predictions = best_model(X_test_tensor)
    test_loss = criterion(predictions, y_test_tensor)    
    y_pred = pd.DataFrame(predictions.cpu().numpy(), columns=['PCE_ave(%)', 'Jsc(mA/cm2)', 'Voc(V)', 'FF'])
    y_test = pd.DataFrame(y_test_tensor.cpu().numpy(), columns=['PCE_ave(%)', 'Jsc(mA/cm2)', 'Voc(V)', 'FF'])

    print(f'Test loss: {test_loss.item()}')
    
    # Define the list of properties to evaluate
    properties = ['PCE_ave(%)', 'Jsc(mA/cm2)', 'Voc(V)', 'FF']

    # Initialize a dictionary to store the metrics
    metrics = {}

    for prop in properties:
        y_true = y_test[prop].to_numpy()        # Convert y_test columns to numpy for compatibility
        y_predicted = y_pred[prop].to_numpy()

        # Pearson correlation
        pearson_corr, _ = pearsonr(y_true, y_predicted)
        # RMSE
        rmse = np.sqrt(mean_squared_error(y_true, y_predicted))
        # MAE
        mae = mean_absolute_error(y_true, y_predicted)
        # R^2
        r2 = r2_score(y_true, y_predicted)

        # Store the metrics
        metrics[prop] = {
            'Pearson': pearson_corr,
            'RMSE': rmse,
            'MAE': mae,
            'R2': r2
            }

    # Print the results
    for prop, values in metrics.items():
        print(f"Metrics for {prop}:")
        for metric_name, value in values.items():
            print(f"  {metric_name}: {value:.4f}")
        print()

# ------------------------------------------------------------------------------
# SAVING RESULTS AS A LATEX TABLE
# ------------------------------------------------------------------------------

# Begin constructing a LaTeX table for metrics
latex_str = r'''\begin{table}[ht]
\centering
\caption{Neural Network Metrics for OPV Properties}
\begin{tabular}{lcccc}
\toprule
Property & Pearson & RMSE & MAE & $R^2$ \\
\midrule
'''

# Loop through each property and add a row with metrics
for prop, vals in metrics.items():
    latex_str += f"{prop} & {vals['Pearson']:.4f} & {vals['RMSE']:.4f} & {vals['MAE']:.4f} & {vals['R2']:.4f} \\\\\n"

# Close the table
latex_str += r'''\bottomrule
\end{tabular}
\end{table}
'''

# Append additional information about hyperparameters and losses as LaTeX comments
latex_str += '\n\n% Best hyperparameters and loss information\n'
latex_str += f"% Best hyperparameters: {study.best_params}\n"
latex_str += f"% Best validation loss: {study.best_value:.6f}\n"
latex_str += f"% Test loss: {test_loss.item():.6f}\n"

# Write the LaTeX string to a file
with open("neural_network_results.tex", "w", encoding="utf-8") as f:
    f.write(latex_str)

# ------------------------------------------------------------------------------
# TRAINING FINAL MODEL ON FULL DATASET AND SAVING IT
# ------------------------------------------------------------------------------

# Convert the entire dataset to PyTorch tensors for training
X_all_tensor = torch.tensor(X.values, dtype=torch.float32).to(device)
y_all_tensor = torch.tensor(y.values, dtype=torch.float32).to(device)
train_dataset = TensorDataset(X_all_tensor, y_all_tensor)
train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)

# Initialize model with best hyperparameters
best_model = ResidualNet(dropout_rate=study.best_params['dropout_rate']).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(
    best_model.parameters(),
    lr=study.best_params['lr'],
    weight_decay=study.best_params['weight_decay']
)

# Full training loop
for epoch in range(epochs):
    best_model.train()
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
        optimizer.zero_grad()
        outputs = best_model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()

# Save the model
torch.save(best_model.state_dict(), 'best_NN_model.pth')
print("Best model trained on full training set and saved as 'best_NN_model.pth'.")
