# -*- coding: utf-8 -*-
"""
Generalized Regression Pipeline for OPV Material Property Prediction
===================================================================

This script explores three main approaches to predict the photovoltaic properties:
    1. **Without fingerprints** – Only numeric molecular descriptors are used.
    2. **Only fingerprints** – Molecular fingerprints are converted to binary vectors and used with Tanimoto similarity.
    3. **All descriptors + fingerprints** – Combines scaled numeric descriptors with the best fingerprint representation.

Machine learning models:
    - k-Nearest Neighbors Regressor (kNN)
    - Kernel Ridge Regression (KRR)
    - Support Vector Regression (SVR)

Features:
    - Train/test splits with scaling
    - Hyperparameter optimization via GridSearchCV and Optuna
    - Tanimoto similarity for fingerprints
    - Combined kernel distances for mixed data
    - Model saving
    - Automated LaTeX table generation for reporting

Author: Victor dos Reis Rodrigues
Created: 2024-10-07
"""

import numpy as np
import pandas as pd
import joblib
import optuna
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr
from scipy.spatial.distance import pdist, squareform
from sklearn.metrics.pairwise import pairwise_kernels
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_absolute_error, r2_score
from collections import defaultdict
import os
import re


# ====================
# Save metrics to LaTeX tables
# ====================
def latex_table(metrics_dict, caption):
    """
    Convert metrics dictionary into a LaTeX table for publication.

    Parameters
    ----------
    metrics_dict : dict
        Dictionary of model performance metrics, where keys are model names
        and values are tuples of (RMSE, MAE, R^2, Pearson).
    caption : str
        Table caption for LaTeX output.

    Returns
    -------
    str
        Formatted LaTeX table string.
    """

    lines = []
    lines.append("\\begin{table}[ht]\n\\centering")
    lines.append("\\begin{tabular}{lcccc}")
    lines.append("\\toprule")
    lines.append("Model & RMSE & MAE & $R^2$ & Pearson \\\\")
    lines.append("\\midrule")
    for model_name, vals in metrics_dict.items():
        rmse, mae, r2, pearson = vals
        lines.append(f"{model_name} & {rmse:.4f} & {mae:.4f} & {r2:.4f} & {pearson:.4f} \\\\")
    lines.append("\\bottomrule")
    lines.append("\\end{tabular}")
    lines.append(f"\\caption{{{caption}}}")
    lines.append("\\end{table}\n")
    return "\n".join(lines)

# ====================
# Data Preprocessing (No Fingerprints)
# ====================
# 1. Load two datasets (FA and NFA) from CSV
# 2. Clean columns, convert datatypes, drop missing values
# 3. Remove fingerprint columns to focus on numeric features only
# 4. Create train/test splits and apply standard scaling

# --- FA dataset (with acceptor info)
df1 = pd.read_csv('final_database_FA.csv', delimiter=',').drop(['ID', 'Nickname', 'Ref'], axis=1)
df1 = df1.rename(columns={'PDI(=Mw/Mn)': 'PDI', 'Monomer(g/m)':'M(g/mol)'})
df1[['Mn(kg/mol)', 'PDI']] = df1[['Mn(kg/mol)', 'PDI']].replace('-', np.nan)
df1[['Mn(kg/mol)', 'PDI']] = df1[['Mn(kg/mol)', 'PDI']].apply(pd.to_numeric, errors='coerce')
df1= df1.drop(['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)'], axis=1)
df1 = df1.dropna()

# --- NFA dataset
df2 = pd.read_csv('final_database_NFA.csv', delimiter=',').drop(['ID', 'Ref', 'n(SMILES)', 'p(SMILES)'], axis=1)
df2 = df2.drop(['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)'], axis=1)
df2 = df2.dropna()

# --- Combine datasets
df = pd.concat([df1, df2], ignore_index=True)

# Define feature matrices and target matrices
X = df[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI']]
y = df[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']]
X_1 = df1[['M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI']]
y_1 = df1[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']]
X_2 = df2[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI']]
y_2 = df2[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.2, random_state=42)

scaler_1 = StandardScaler()
X_train_scaled_1 = scaler_1.fit_transform(X_train_1)
X_test_scaled_1 = scaler_1.transform(X_test_1)

X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.2, random_state=42)

scaler_2 = StandardScaler()
X_train_scaled_2 = scaler_2.fit_transform(X_train_2)
X_test_scaled_2 = scaler_2.transform(X_test_2)

def evaluate_model(model, X_test, y_test):
    """
    Parameters
    ----------
    model : estimator
        Trained scikit-learn compatible regressor.
    X_test : array-like
        Test feature set.
    y_test : array-like
        True target values.

    Returns
    -------
    tuple
        (RMSE, MAE, R², Pearson correlation coefficient)
    """

    y_pred = model.predict(X_test)
    
    if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:
        y_pred = y_pred.ravel()
    if len(y_test.shape) == 2 and y_test.shape[1] == 1:
        y_test = y_test.values.ravel()
    
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    pearson_corr, _ = pearsonr(y_test, y_pred)
    print(f'Test MSE: {mse:.4f}')
    print(f'Test RMSE: {rmse:.4f}')
    print(f'Test MAE: {mae:.4f}')
    print(f'Test R^2: {r2:.4f}')
    print(f'Pearson Correlation Coefficient: {pearson_corr:.4f}')
    
    return rmse, mae, r2, pearson_corr

def collect_metrics(model, X_test, y_test, model_name, metrics_dict):
    """
    Wrapper to evaluate model and store metrics in a shared dictionary.

    Parameters
    ----------
    model : estimator
        Trained regressor.
    X_test : array-like
        Test features.
    y_test : array-like
        True target values.
    model_name : str
        Name of the model (for labeling results).
    metrics_dict : dict
        Dictionary where metrics will be stored.
    """
    rmse, mae, r2, pearson = evaluate_model(model, X_test, y_test)
    metrics_dict[model_name] = (rmse, mae, r2, pearson)

def train_models(X_train, y_train, X_test, y_test, save_prefix=None):
    knn_params = {
        'n_neighbors': [3,5,7,9,11,13,15],
        'weights': ['uniform', 'distance'],
        'p': [1, 2]  # 1: Manhattan, 2: Euclidean distance
        }

    krr_params = {
        'alpha': [0.01, 0.1, 1.0, 10],
        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
        'degree': [2, 3, 4]  # Only applicable if 'poly' kernel is used
        }

    svr_params = {
        'C': [0.1, 1, 10, 100],
        'epsilon': [0.01, 0.1, 1],
        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
        'degree': [2, 3, 4]  # Only applicable if 'poly' kernel is used
        }

    knn = KNeighborsRegressor()
    krr = KernelRidge()
    svr = SVR()

    knn_search = GridSearchCV(knn, knn_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
    krr_search = GridSearchCV(krr, krr_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
    svr_search = GridSearchCV(svr, svr_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

    knn_search.fit(X_train, y_train.values.ravel())
    krr_search.fit(X_train, y_train.values.ravel())
    svr_search.fit(X_train, y_train.values.ravel())

    print("Best k-NN parameters:", knn_search.best_params_)
    evaluate_model(knn_search.best_estimator_, X_test, y_test)
    print("Best Kernel Ridge parameters:", krr_search.best_params_)
    evaluate_model(krr_search.best_estimator_, X_test, y_test)    
    print("Best SVR parameters:", svr_search.best_params_)
    evaluate_model(svr_search.best_estimator_, X_test, y_test)
    
    # Retrain on the entire dataset (train + test)
    X_all = np.concatenate([X_train, X_test])
    y_all = np.concatenate([y_train, y_test])

    knn_final = KNeighborsRegressor(**knn_search.best_params_)
    krr_final = KernelRidge(**krr_search.best_params_)
    svr_final = SVR(**svr_search.best_params_)

    knn_final.fit(X_all, y_all.ravel())
    krr_final.fit(X_all, y_all.ravel())
    svr_final.fit(X_all, y_all.ravel())

     # Save models if save_prefix is provided
    if save_prefix is not None:
        os.makedirs("saved_models", exist_ok=True)
        joblib.dump(knn_final, f"saved_models/{save_prefix}_knn.joblib")
        joblib.dump(krr_final, f"saved_models/{save_prefix}_krr.joblib")
        joblib.dump(svr_final, f"saved_models/{save_prefix}_svr.joblib")

    return knn_final, krr_final, svr_final

def run_all_models(train_func, X_train, y_train, X_test, y_test, dataset, metrics_dict):
    targets = [
        ("PCE_ave(%)", "PCE"),
        ("Jsc(mA/cm2)", "Jsc"),
        ("FF", "FF"),
        ("Voc(V)", "Voc")
    ]
    for col, short in targets:
        print(f"Prediction of {short}")
        # Pass save_prefix to train_models for saving
        models = train_func(X_train, y_train[[col]], X_test, y_test[[col]], save_prefix=f"no_fp_{dataset}_{short}")
        if isinstance(models, tuple):
            names = ["kNN", "KRR", "SVR"]
            for m, name in zip(models, names):
                collect_metrics(m, X_test, y_test[[col]], f"{dataset} {name} {short}", metrics_dict)
        else:
            collect_metrics(models, X_test, y_test[[col]], f"{dataset} {train_func.__name__} {short}", metrics_dict)

print("Not considering fingerprints\n")
metrics_nf = defaultdict(tuple)
print("\nResults FA\n")
run_all_models(train_models, X_train_scaled_1, y_train_1, X_test_scaled_1, y_test_1, "FA", metrics_nf)
print("\nResults NFA\n")
run_all_models(train_models, X_train_scaled_2, y_train_2, X_test_scaled_2, y_test_2, "NFA", metrics_nf)
print("\nResults FA+NFA\n")
run_all_models(train_models, X_train_scaled, y_train, X_test_scaled, y_test, "FA+NFA", metrics_nf)

# Saving all metrics to a LaTeX table
with open("metrics_not_fingerprints.tex", "w", encoding="utf-8") as f:
    f.write(latex_table(metrics_nf, "Results without fingerprints"))

# ====================
# Only considering fingerprints
# ====================
print("\nConsidering Only fingerprints\n")

def is_binary(fp):
    """ It checks if a fingerprint string contains only binary digits """
    return isinstance(fp, str) and all(char in '01' for char in fp)

# --- Load FA dataset and keep only valid binary fingerprints
df1 = pd.read_csv('final_database_FA.csv', delimiter=',').drop(['ID', 'Nickname', 'Ref'], axis=1)
df1 = df1[df1[['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)']].apply(lambda x: x.map(is_binary)).all(axis=1)]
# Concatenate donor/acceptor fingerprints to form complete molecular representation
df1['CDK_fp']    = df1['n(CDK Fingerprint)'] + df1['p(CDK Fingerprint)']
df1['RDKit_fp']  = df1['n(RDKit Fingerprint)'] + df1['p(RDKit Fingerprint)']
df1['Morgan_fp'] = df1['n(Morgan Fingerprint)'] + df1['p(Morgan Fingerprint)']
# Keep only fingerprints and target columns
df1 = df1[['CDK_fp', 'RDKit_fp', 'Morgan_fp', 'PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']].reset_index(drop=True)

# --- Load NFA dataset and process similarly
df2 = pd.read_csv('final_database_NFA.csv', delimiter=',').drop(['ID', 'Ref', 'n(SMILES)', 'p(SMILES)'], axis=1)
df2 = df2[df2[['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)']].apply(lambda x: x.map(is_binary)).all(axis=1)]
df2['CDK_fp']    = df2['n(CDK Fingerprint)'] + df2['p(CDK Fingerprint)']
df2['RDKit_fp']  = df2['n(RDKit Fingerprint)'] + df2['p(RDKit Fingerprint)']
df2['Morgan_fp'] = df2['n(Morgan Fingerprint)'] + df2['p(Morgan Fingerprint)']
df2 = df2[['CDK_fp', 'RDKit_fp', 'Morgan_fp', 'PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']].reset_index(drop=True)

# Combine FA and NFA datasets
df = pd.concat([df1, df2], ignore_index=True)

df.rename(columns={"Jsc(mA/cm2)": "Jsc"}, inplace=True)
df1.rename(columns={"Jsc(mA/cm2)": "Jsc"}, inplace=True)
df2.rename(columns={"Jsc(mA/cm2)": "Jsc"}, inplace=True)

# Define features (fingerprints) and targets
X = df[['CDK_fp', 'RDKit_fp', 'Morgan_fp']]
X1 = df1[['CDK_fp', 'RDKit_fp', 'Morgan_fp']]
X2 = df2[['CDK_fp', 'RDKit_fp', 'Morgan_fp']]
y = df[['PCE_ave(%)', 'Jsc', 'FF', 'Voc(V)']]
y1 = df1[['PCE_ave(%)', 'Jsc', 'FF', 'Voc(V)']]
y2 = df2[['PCE_ave(%)', 'Jsc', 'FF', 'Voc(V)']]

fp_train, fp_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
fp_train_1, fp_test_1, y_train_1, y_test_1 = train_test_split(X1, y1, test_size=0.2, random_state=42)
fp_train_2, fp_test_2, y_train_2, y_test_2 = train_test_split(X2, y2, test_size=0.2, random_state=42)

def convert_to_binary_vector(fp_str):
    # Check if all characters in the string are '0' or '1'
    if all(bit in '01' for bit in fp_str):
        return [int(bit) for bit in fp_str]
    else:
        raise ValueError(f"Invalid fingerprint format: {fp_str}")

# Convert fingerprints into NumPy arrays
try:
    train_binary_vectors_mor = np.array([convert_to_binary_vector(fp) for fp in fp_train['Morgan_fp']])
    train_binary_vectors_cdk = np.array([convert_to_binary_vector(fp) for fp in fp_train['CDK_fp']])
    train_binary_vectors_rdk = np.array([convert_to_binary_vector(fp) for fp in fp_train['RDKit_fp']])
    
    train_binary_vectors_mor_1 = np.array([convert_to_binary_vector(fp) for fp in fp_train_1['Morgan_fp']])
    train_binary_vectors_cdk_1 = np.array([convert_to_binary_vector(fp) for fp in fp_train_1['CDK_fp']])
    train_binary_vectors_rdk_1 = np.array([convert_to_binary_vector(fp) for fp in fp_train_1['RDKit_fp']])
    
    train_binary_vectors_mor_2 = np.array([convert_to_binary_vector(fp) for fp in fp_train_2['Morgan_fp']])
    train_binary_vectors_cdk_2 = np.array([convert_to_binary_vector(fp) for fp in fp_train_2['CDK_fp']])
    train_binary_vectors_rdk_2 = np.array([convert_to_binary_vector(fp) for fp in fp_train_2['RDKit_fp']] )  
except ValueError as e:
    print(e)
    
try:
    test_binary_vectors_mor = np.array([convert_to_binary_vector(fp) for fp in fp_test['Morgan_fp']])
    test_binary_vectors_cdk = np.array([convert_to_binary_vector(fp) for fp in fp_test['CDK_fp']])
    test_binary_vectors_rdk = np.array([convert_to_binary_vector(fp) for fp in fp_test['RDKit_fp']])
    
    test_binary_vectors_mor_1 = np.array([convert_to_binary_vector(fp) for fp in fp_test_1['Morgan_fp']])
    test_binary_vectors_cdk_1 = np.array([convert_to_binary_vector(fp) for fp in fp_test_1['CDK_fp']])
    test_binary_vectors_rdk_1 = np.array([convert_to_binary_vector(fp) for fp in fp_test_1['RDKit_fp']])

    test_binary_vectors_mor_2 = np.array([convert_to_binary_vector(fp) for fp in fp_test_2['Morgan_fp']])
    test_binary_vectors_cdk_2 = np.array([convert_to_binary_vector(fp) for fp in fp_test_2['CDK_fp']])
    test_binary_vectors_rdk_2 = np.array([convert_to_binary_vector(fp) for fp in fp_test_2['RDKit_fp']])
except ValueError as e:
    print(e)

metrics_fp = defaultdict(tuple)

# --- Tanimoto similarity matrix---
def tanimoto_coeff_similarity_matrix(X, Y):
    intersection = np.dot(X, Y.T)
    union = np.sum(X, axis=1)[:, None] + np.sum(Y, axis=1) - intersection
    return intersection / union  # similarity

# --- Decide distance or kernel ---
def get_kernel_or_distance(model_type, X, Y):
    """Return a precomputed kernel matrix (similarity) or distance matrix depending on model."""
    if model_type == KNeighborsRegressor:
        return 1 - tanimoto_coeff_similarity_matrix(X, Y)  # distance
    else:
        return tanimoto_coeff_similarity_matrix(X, Y)      # similarity

# --- Training function ---
def train_model(model_type, binary_vectors, target_values, test_vectors, y_test, param_grid, scoring='neg_mean_squared_error', cv=5, save_prefix=None):
    """Perform grid search and retrain models using precomputed similarity/distance matrices."""

    models = {
        fp: (
            model_type(metric='precomputed') if model_type == KNeighborsRegressor
            else model_type(kernel='precomputed')
        )
        for fp in binary_vectors
    }
    
    results = {}
    retrained_models = {}
    
    for fp in models:
        X_train = get_kernel_or_distance(model_type, binary_vectors[fp], binary_vectors[fp])
        X_test  = get_kernel_or_distance(model_type, test_vectors[fp], binary_vectors[fp])
        
        search = GridSearchCV(models[fp], param_grid, cv=cv, scoring=scoring, n_jobs=-1)
        search.fit(X_train, target_values)
        
        print(f"{model_type.__name__}: Best Parameters for {fp}: {search.best_params_}")
        best_model = search.best_estimator_       
        
        # Evaluation function (user must define)
        evaluate_model(best_model, X_test, y_test)
        results[fp] = (best_model, X_test)

        # Retrain on full dataset (train + test)
        X_all = np.concatenate([binary_vectors[fp], test_vectors[fp]])
        y_all = np.concatenate([target_values, y_test])
        K_all = get_kernel_or_distance(model_type, X_all, X_all)
        
        final_model = (
            model_type(metric='precomputed') if model_type == KNeighborsRegressor
            else model_type(kernel='precomputed')
        )
        final_model.set_params(**search.best_params_)
        final_model.fit(K_all, y_all)
        
        retrained_models[fp] = final_model

        # Save the model if needed
        if save_prefix is not None:
            os.makedirs("saved_models", exist_ok=True)
            filename = f"saved_models/{save_prefix}_{fp}.joblib"
            joblib.dump(final_model, filename)
    
    return results, retrained_models

def collect_all_metrics(results, y_test, prefix, metrics_fp):
    for fp, (model, X_test) in results.items():
        name = f"{prefix} {fp} {model.__class__.__name__}"
        collect_metrics(model, X_test, y_test, name, metrics_fp)

experiments = [
    ("FA", train_binary_vectors_mor_1, train_binary_vectors_cdk_1, train_binary_vectors_rdk_1,
     test_binary_vectors_mor_1, test_binary_vectors_cdk_1, test_binary_vectors_rdk_1, y_train_1, y_test_1),
    ("NFA", train_binary_vectors_mor_2, train_binary_vectors_cdk_2, train_binary_vectors_rdk_2,
     test_binary_vectors_mor_2, test_binary_vectors_cdk_2, test_binary_vectors_rdk_2, y_train_2, y_test_2),
    ("FA+NFA", train_binary_vectors_mor, train_binary_vectors_cdk, train_binary_vectors_rdk,
     test_binary_vectors_mor, test_binary_vectors_cdk, test_binary_vectors_rdk, y_train, y_test)
]

targets = ['PCE_ave(%)', 'Jsc', 'FF', 'Voc(V)']
models = [
    (KNeighborsRegressor, {'n_neighbors': [3,5,7,9,11,13,15], 'weights': ['uniform', 'distance']}),
    (SVR, {'C': [0.1, 1, 10, 100], 'epsilon': [0.01, 0.1, 1]}),
    (KernelRidge, {'alpha': [0.1, 1, 10, 100]} )
]

for model_type, param_grid in models:
    for label_dataset, mor_train, cdk_train, rdk_train, mor_test, cdk_test, rdk_test, y_train, y_test in experiments:
        print(f"\nResults {label_dataset}\n")
        for target in targets:
            try:
                print(f"Prediction of {target} with {model_type.__name__}")
                binary_vectors = {'Morgan': mor_train,'CDK': cdk_train,'RDKit': rdk_train}
                test_vectors = {'Morgan': mor_test,'CDK': cdk_test,'RDKit': rdk_test}

                save_prefix = f"only_fp_{label_dataset}_{model_type.__name__}_{target}"
                results, retrained_models = train_model(model_type, binary_vectors, y_train[target], test_vectors, y_test[target], param_grid, save_prefix=save_prefix)
                collect_all_metrics(results, y_test[target], f"{label_dataset} {model_type.__name__} {target}", metrics_fp)
            except Exception as e:
                print(f"Error training {model_type.__name__} on {label_dataset} - {target}: {e}")

# Saving all metrics to a LaTeX table
with open("metrics_only_fingerprints.tex", "w", encoding="utf-8") as f:
    f.write(latex_table(metrics_fp, "Results using only fingerprints"))


# ====================
# Considering all descriptors
# ====================
print("\nConsidering all float descriptors and the best performing fingerprint\n")

def is_binary(fp):
    return isinstance(fp, str) and all(char in '01' for char in fp)

def convert_to_binary_vector(fp_str):
    # Check if all characters in the string are '0' or '1'
    if all(bit in '01' for bit in fp_str):
        return [int(bit) for bit in fp_str]
    else:
        raise ValueError(f"Invalid fingerprint format: {fp_str}")

# Data loading and preprocessing follows the same steps as before, but this time
# we retain both numeric descriptors and Morgan fingerprints. After scaling numeric
# features, binary fingerprint vectors are concatenated to form final input arrays.

df1 = pd.read_csv('final_database_FA.csv', delimiter=',').drop(['ID', 'Nickname', 'Ref'], axis=1)
df1 = df1.rename(columns={'PDI(=Mw/Mn)': 'PDI', 'Monomer(g/m)':'M(g/mol)'})
df1[['Mn(kg/mol)', 'PDI']] = df1[['Mn(kg/mol)', 'PDI']].replace('-', np.nan)
df1[['Mn(kg/mol)', 'PDI']] = df1[['Mn(kg/mol)', 'PDI']].apply(pd.to_numeric, errors='coerce')
df1 = df1.dropna(subset=['Mn(kg/mol)', 'PDI'])
df1 = df1[df1[['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)']].apply(lambda x: x.map(is_binary)).all(axis=1)]
df1 = df1.reset_index(drop=True)
df1['CDK_fp'] = df1['n(CDK Fingerprint)'] + df1['p(CDK Fingerprint)']
df1['RDKit_fp'] = df1['n(RDKit Fingerprint)'] + df1['p(RDKit Fingerprint)']
df1['Morgan_fp'] = df1['n(Morgan Fingerprint)'] + df1['p(Morgan Fingerprint)']
df1 = df1.drop(['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)'], axis=1)
df1 = df1.dropna()

df2 = pd.read_csv('final_database_NFA.csv', delimiter=',').drop(['ID', 'Ref', 'n(SMILES)', 'p(SMILES)'], axis=1)
df2 = df2[df2[['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)']].apply(lambda x: x.map(is_binary)).all(axis=1)]
df2 = df2.reset_index(drop=True)
df2['CDK_fp'] = df2['n(CDK Fingerprint)'] + df2['p(CDK Fingerprint)']
df2['RDKit_fp'] = df2['n(RDKit Fingerprint)'] + df2['p(RDKit Fingerprint)']
df2['Morgan_fp'] = df2['n(Morgan Fingerprint)'] + df2['p(Morgan Fingerprint)']
df2 = df2.drop(['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)'], axis=1)
df2 = df2.dropna()

df = pd.concat([df1, df2], ignore_index=True)

X = df[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI', 'CDK_fp', 'RDKit_fp', 'Morgan_fp']]
y = df[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']]
X1 = df1[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI', 'CDK_fp', 'RDKit_fp', 'Morgan_fp']]
y1 = df1[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']]
X2 = df2[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI', 'CDK_fp', 'RDKit_fp', 'Morgan_fp']]
y2 = df2[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X1, y1, test_size=0.2, random_state=42)
X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X2, y2, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI']])
X_test_scaled = scaler.transform(X_test[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI']])
X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI'])
X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI'])

scaler = StandardScaler()
X_train_scaled_1 = scaler.fit_transform(X_train_1[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI']])
X_test_scaled_1 = scaler.transform(X_test_1[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI']])
X_train_scaled_df_1 = pd.DataFrame(X_train_scaled_1, columns=['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI'])
X_test_scaled_df_1 = pd.DataFrame(X_test_scaled_1, columns=['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI'])

scaler = StandardScaler()
X_train_scaled_2 = scaler.fit_transform(X_train_2[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI']])
X_test_scaled_2 = scaler.transform(X_test_2[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI']])
X_train_scaled_df_2 = pd.DataFrame(X_train_scaled_2, columns=['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI'])
X_test_scaled_df_2 = pd.DataFrame(X_test_scaled_2, columns=['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI'])

used_fingerprint = 'Morgan_fp'

try:
    binary_vectors_train = X_train[used_fingerprint].apply(convert_to_binary_vector)
    binary_vectors_test  = X_test[used_fingerprint].apply(convert_to_binary_vector)
    
    binary_vectors_train_1 = X_train_1[used_fingerprint].apply(convert_to_binary_vector)
    binary_vectors_test_1  = X_test_1[used_fingerprint].apply(convert_to_binary_vector)    
    
    binary_vectors_train_2 = X_train_2[used_fingerprint].apply(convert_to_binary_vector)
    binary_vectors_test_2  = X_test_2[used_fingerprint].apply(convert_to_binary_vector)
except ValueError as e:
    print(e)
    
# Create a DataFrame from the list of binary vectors
binary_df_train = pd.DataFrame(binary_vectors_train.tolist(), columns=[f'bit_{i}' for i in range(len(binary_vectors_train.iloc[0]))])
binary_df_test = pd.DataFrame(binary_vectors_test.tolist(), columns=[f'bit_{i}' for i in range(len(binary_vectors_test.iloc[0]))])

binary_df_train_1 = pd.DataFrame(binary_vectors_train_1.tolist(), columns=[f'bit_{i}' for i in range(len(binary_vectors_train_1.iloc[0]))])
binary_df_test_1 = pd.DataFrame(binary_vectors_test_1.tolist(), columns=[f'bit_{i}' for i in range(len(binary_vectors_test_1.iloc[0]))])

binary_df_train_2 = pd.DataFrame(binary_vectors_train_2.tolist(), columns=[f'bit_{i}' for i in range(len(binary_vectors_train_2.iloc[0]))])
binary_df_test_2 = pd.DataFrame(binary_vectors_test_2.tolist(), columns=[f'bit_{i}' for i in range(len(binary_vectors_test_2.iloc[0]))])

X_train_final = pd.concat([X_train_scaled_df, binary_df_train], axis=1)
X_test_final = pd.concat([X_test_scaled_df, binary_df_test], axis=1)

X_train_final_1 = pd.concat([X_train_scaled_df_1, binary_df_train_1], axis=1)
X_test_final_1 = pd.concat([X_test_scaled_df_1, binary_df_test_1], axis=1)

X_train_final_2 = pd.concat([X_train_scaled_df_2, binary_df_train_2], axis=1)
X_test_final_2 = pd.concat([X_test_scaled_df_2, binary_df_test_2], axis=1)

# Convert DataFrames to NumPy arrays
X_train_np = X_train_final.values
X_test_np = X_test_final.values

X_train_np_1 = X_train_final_1.values
X_test_np_1 = X_test_final_1.values

X_train_np_2 = X_train_final_2.values
X_test_np_2 = X_test_final_2.values

# --- Custom distance and kernel functions
def tanimoto_coeff(x, y):
    """Compute Tanimoto distance between two binary vectors."""
    intersection = np.sum(np.logical_and(x, y))
    union = np.sum(x) + np.sum(y) - intersection
    return 0.0 if union == 0 else 1 - intersection / union

def custom_distance(vector_1, vector_2, gamma_1, gamma_2):
    """Combined distance metric using Euclidean for numeric and Tanimoto for fingerprints."""
    # vector_1 and vector_2 must be NumPy arrays for indexing
    vector_1 = np.asarray(vector_1)
    vector_2 = np.asarray(vector_2)

    dist_1 = np.linalg.norm(vector_1[0:10] - vector_2[0:10])
    dist_2 = tanimoto_coeff(vector_1[10:], vector_2[10:])
    
    return gamma_1 * dist_1 + gamma_2 * dist_2

def tanimoto_coeff_matrix(X, Y):
    intersection = np.dot(X, Y.T)
    union = np.sum(X, axis=1)[:, None] + np.sum(Y, axis=1) - intersection
    return 1 - (intersection / union)

def custom_kernel(X, Y, gamma_1, gamma_2, kernel_type='rbf', degree=3):
    """Construct custom kernel matrix using Euclidean + Tanimoto similarity."""
    # Euclidean distance for the first part
    dist_1 = np.linalg.norm(X[:, :10, None] - Y[:, :10].T, axis=1)
    
    # Tanimoto distance for the second part
    dist_2 = tanimoto_coeff_matrix(X[:, 10:], Y[:, 10:])
    
    if kernel_type == 'rbf':
        return np.exp(-(gamma_1 * (dist_1**2) + gamma_2 * (dist_2**2) ))
    elif kernel_type == 'linear':
        return gamma_1 * dist_1 + gamma_2 * dist_2
    elif kernel_type == 'polynomial':
        return (1 + (gamma_1 * dist_1 + gamma_2 * dist_2)**2)**degree
    else:
        raise ValueError("Unsupported kernel type. Choose 'rbf', 'linear', or 'polynomial'.")

metrics_all = defaultdict(tuple)

def collect_metrics(model, X_test, y_test, model_name, metrics_dict):
    rmse, mae, r2, pearson = evaluate_model(model, X_test, y_test)
    metrics_dict[model_name] = (rmse, mae, r2, pearson)

def get_model(name, params):
    if name == 'knn':
        return KNeighborsRegressor(n_neighbors=params['n_neighbors'], metric='precomputed')
    elif name == 'krr':
        return KernelRidge(kernel='precomputed', alpha=params['alpha'])
    elif name == 'svr':
        return SVR(kernel='precomputed', C=params['C'], epsilon=params['epsilon'])
    else:
        raise ValueError(f"Unsupported model: {name}")

# ====================
# --- Optuna Hyperparameter Optimization
# ====================
def train_model(X_train, y_train, X_test, y_test, model_name, target_name, data_label, n_trials=150, save_prefix=None):
    """Optimize kernel parameters and model hyperparameters using Optuna."""
    def objective(trial):
        gamma_1 = trial.suggest_float('gamma_1', 0.1 if model_name == 'knn' else 1e-3, 2.0 if model_name == 'knn' else 10.0)
        gamma_2 = trial.suggest_float('gamma_2', 0.1 if model_name == 'knn' else 1e-3, 2.0 if model_name == 'knn' else 10.0)
        kernel_type = trial.suggest_categorical('kernel_type', ['rbf', 'linear', 'polynomial'])
        degree = trial.suggest_int('degree', 2, 5) if kernel_type == 'polynomial' else None

        if model_name == 'knn':
            n_neighbors = trial.suggest_int('n_neighbors', 1, 20)
            params = {'n_neighbors': n_neighbors}
        elif model_name == 'krr':
            alpha = trial.suggest_float('alpha', 1e-4, 1e1, log=True)
            params = {'alpha': alpha}
        elif model_name == 'svr':
            C = trial.suggest_float('C', 1e-2, 1e2, log=True)
            epsilon = trial.suggest_float('epsilon', 1e-4, 1e-1, log=True)
            params = {'C': C, 'epsilon': epsilon}

        K_train = custom_kernel(X_train, X_train, gamma_1, gamma_2, kernel_type=kernel_type, degree=degree)
        model = get_model(model_name, params)
        
        cv = KFold(n_splits=5)
        scores = cross_val_score(model, K_train, y_train.squeeze(), cv=cv, scoring='neg_mean_squared_error')
        return -np.mean(scores)

    # Run the optimization
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=n_trials)

    best_params = study.best_params
    print(f"Best params for {model_name.upper()} predicting {target_name}: {best_params}")

    # Train the final model with the optimized parameters
    gamma_1 = best_params['gamma_1']
    gamma_2 = best_params['gamma_2']
    kernel_type = best_params['kernel_type']
    degree = best_params.get('degree', None) if kernel_type == 'polynomial' else None

    if model_name == 'knn':
        params = {'n_neighbors': best_params['n_neighbors']}
    elif model_name == 'krr':
        params = {'alpha': best_params['alpha']}
    elif model_name == 'svr':
        params = {'C': best_params['C'], 'epsilon': best_params['epsilon']}

    model_final = get_model(model_name, params)

    K_train_final = custom_kernel(X_train, X_train, gamma_1, gamma_2, kernel_type=kernel_type, degree=degree)
    model_final.fit(K_train_final, y_train.squeeze())

    K_test_final = custom_kernel(X_test, X_train, gamma_1, gamma_2, kernel_type=kernel_type, degree=degree)

    # Evaluate and collect metrics using kernel matrices
    collect_metrics(model_final, K_test_final, y_test, f"{data_label} {model_name.upper()} {target_name}", metrics_all)

    # Retrain final model on ALL data (train + test)
    X_all = np.vstack([X_train, X_test])
    y_all = np.concatenate([y_train, y_test])

    model_final_all = get_model(model_name, params)
    K_all = custom_kernel(X_all, X_all, gamma_1, gamma_2, kernel_type=kernel_type, degree=degree)
    model_final_all.fit(K_all, y_all.squeeze())

    # Save the retrained model if save_prefix is provided
    if save_prefix is not None:
        os.makedirs("saved_models", exist_ok=True)
        safe_target_name = re.sub(r'[^A-Za-z0-9._-]', '_', target_name)
        filename = f"saved_models/{save_prefix}_{model_name}_{safe_target_name}.joblib"
        joblib.dump(model_final_all, filename)

    return model_final_all

# Define your datasets dictionary and targets list as before:
datasets = {
    "FA": (X_train_np_1, y_train_1, X_test_np_1, y_test_1),
    "NFA": (X_train_np_2, y_train_2, X_test_np_2, y_test_2),
    "FA+NFA": (X_train_np, y_train, X_test_np, y_test),
}

targets = ['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']

# --- Final Execution Loop
# Iterate through datasets and targets, train models, and save metrics table.
for data_label, (X_tr, y_tr, X_te, y_te) in datasets.items():
    print(f"\nResults {data_label}\n")
    for model_name in ['knn', 'krr', 'svr']:
        print(f"Training {model_name.upper()} models...")
        for target in targets:
            print(f"Prediction of {target} with {model_name.upper()}")
            save_prefix = f"all_desc_{data_label}_{used_fingerprint}"
            train_model(X_tr, y_tr[target], X_te, y_te[target], model_name, target, data_label, save_prefix=save_prefix)

with open(f"metrics_all_descriptors_{used_fingerprint}.tex", "w", encoding="utf-8") as f:
    f.write(latex_table(metrics_all, "Results using all descriptors"))
