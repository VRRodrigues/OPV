# -*- coding: utf-8 -*-
"""
Machine Learning Training Script for the Tree Models

Author: vicre
Date: Wed Jul 30 2025

This script loads two datasets (FA + NFA) containing molecular descriptors and
fingerprints, preprocesses them, trains multiple regression models (Random Forest,
AdaBoost, CatBoost, XGBoost, LightGBM) with hyperparameter optimization using
Bayesian Search or Optuna, evaluates the models, saves feature importance plots,
and exports performance metrics to LaTeX tables. The script supports training
both on standard descriptors and compressed fingerprints.

Dependencies:
- numpy, pandas, scikit-learn, skopt, matplotlib, seaborn, scipy
- optuna, joblib, catboost, xgboost, lightgbm, torch
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from skopt import BayesSearchCV
from skopt.space import Integer, Real, Categorical
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import os
import optuna
import joblib


# ==== Metrics Saving Utilities ====
def save_metrics_latex(metrics_dict, filename, dataset_label):
    """
    Save evaluation metrics to a LaTeX table file.

    Parameters:
    - metrics_dict: Dict with keys (model_name, dataset_name, target) and values (rmse, pearson_corr, r2, mae)
    - filename: Output LaTeX file path
    - dataset_label: Caption/label for the LaTeX table
    """
    with open(filename, "w") as f:
        f.write("\\begin{table}[ht]\n\\centering\n")
        f.write("\\begin{tabular}{l l l c c c c}\n")
        f.write("\\toprule\n")
        f.write("Model & Dataset & Target & RMSE & MAE & Pearson & $R^2$ \\\\\n")
        f.write("\\midrule\n")
        for (model_name, dataset_name, target), (rmse, pearson_corr, r2, mae) in metrics_dict.items():
            f.write(f"{model_name} & {dataset_name} & {target} & {rmse:.4f} & {mae:.4f} & {pearson_corr:.4f} & {r2:.4f} \\\\\n")
        f.write("\\bottomrule\n")
        f.write("\\end{tabular}\n")
        f.write(f"\\caption{{Metrics for {dataset_label}}}\n")
        f.write("\\end{table}\n")

# ==== Metrics Collection Dictionaries ====
metrics_others = {}
metrics_compressed_fp = {}

# ==== Data Preprocessing ====
def load_data(data = "others"):
    """
    Load and preprocess datasets.

    Parameters:
    - data: 'others' for descriptor-based datasets, 'compressed_fingerprints' for compressed fingerprints

    Returns:
    - Multiple tuples for train/test splits and feature/target matrices
    """

    if (data == "others"):
        # Load FA dataset
        df1 = pd.read_csv('final_database_FA.csv', delimiter=',').drop(['ID', 'Nickname', 'Ref'], axis=1)
        df1 = df1.rename(columns={'PDI(=Mw/Mn)': 'PDI', 'Monomer(g/m)':'M(g/mol)'})
        df1[['Mn(kg/mol)', 'PDI']] = df1[['Mn(kg/mol)', 'PDI']].replace('-', np.nan)
        df1[['Mn(kg/mol)', 'PDI']] = df1[['Mn(kg/mol)', 'PDI']].apply(pd.to_numeric, errors='coerce')
        df1= df1.drop(['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)'], axis=1)
        df1 = df1.dropna()

        # Compute electronic deltas
        df1['-delta_HL_nn'] = df1['-HOMO_n(eV)'] - df1['-LUMO_n(eV)']
        df1['-delta_HL_pp'] = df1['-HOMO_p(eV)'] - df1['-LUMO_p(eV)']
        df1['-delta_HH_np'] = df1['-HOMO_n(eV)'] - df1['-HOMO_p(eV)']
        df1['-delta_LL_np'] = df1['-LUMO_n(eV)'] - df1['-LUMO_p(eV)']
        df1['-delta_HL_np'] = df1['-HOMO_n(eV)'] - df1['-LUMO_p(eV)']
        df1['-delta_LH_np'] = df1['-LUMO_n(eV)'] - df1['-HOMO_p(eV)']

        # Load NFA dataset
        df2 = pd.read_csv('final_database_NFA.csv', delimiter=',').drop(['ID', 'Ref', 'n(SMILES)', 'p(SMILES)'], axis=1)
        df2 = df2.drop(['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)'], axis=1)
        df2 = df2.dropna()

        # Compute electronic deltas
        df2['-delta_HL_nn'] = df2['-HOMO_n(eV)'] - df2['-LUMO_n(eV)']
        df2['-delta_HL_pp'] = df2['-HOMO_p(eV)'] - df2['-LUMO_p(eV)']
        df2['-delta_HH_np'] = df2['-HOMO_n(eV)'] - df2['-HOMO_p(eV)']
        df2['-delta_LL_np'] = df2['-LUMO_n(eV)'] - df2['-LUMO_p(eV)']
        df2['-delta_HL_np'] = df2['-HOMO_n(eV)'] - df2['-LUMO_p(eV)']
        df2['-delta_LH_np'] = df2['-LUMO_n(eV)'] - df2['-HOMO_p(eV)']

        # Combine datasets
        df = pd.concat([df1, df2], ignore_index=True)
    
        X = df[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI',
                '-delta_HL_nn', '-delta_HL_pp', '-delta_HH_np', '-delta_LL_np', '-delta_HL_np', '-delta_LH_np']]
        y = df[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']]
        X_1 = df1[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI',
                   '-delta_HL_pp', '-delta_HH_np', '-delta_LL_np', '-delta_HL_np', '-delta_LH_np']]
        y_1 = df1[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']]
        X_2 = df2[['-HOMO_n(eV)', '-LUMO_n(eV)', 'Eg_n(eV)', 'M(g/mol)', '-HOMO_p(eV)', '-LUMO_p(eV)', 'Eg_p(eV)', 'Mw(kg/mol)', 'Mn(kg/mol)', 'PDI',
                   '-delta_HL_nn', '-delta_HL_pp', '-delta_HH_np', '-delta_LL_np', '-delta_HL_np', '-delta_LH_np']]
        y_2 = df2[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']]
    
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.2, random_state=42)
        X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.2, random_state=42)
        
        return X, y, X_1, y_1, X_2, y_2, X_train, X_test, y_train, y_test, X_train_1, X_test_1, y_train_1, y_test_1, X_train_2, X_test_2, y_train_2, y_test_2
    
    elif (data == 'compressed_fingerprints'):        
        def is_binary(fp):
            """Check if fingerprint string contains only 0/1 characters."""
            return isinstance(fp, str) and all(char in '01' for char in fp)
        
        # Load FA dataset with fingerprints
        df1 = pd.read_csv('final_database_FA.csv', delimiter=',').drop(['ID', 'Nickname', 'Ref'], axis=1)
        df1 = df1[df1[['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)']].apply(lambda x: x.map(is_binary)).all(axis=1)]
        df1['CDK_fp']    = df1['n(CDK Fingerprint)'] + df1['p(CDK Fingerprint)']
        df1['RDKit_fp']  = df1['n(RDKit Fingerprint)'] + df1['p(RDKit Fingerprint)']
        df1['Morgan_fp'] = df1['n(Morgan Fingerprint)'] + df1['p(Morgan Fingerprint)']
        df1 = df1[['CDK_fp', 'RDKit_fp', 'Morgan_fp', 'PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']].reset_index(drop=True)

         # Load NFA dataset with fingerprints
        df2 = pd.read_csv('final_database_NFA.csv', delimiter=',').drop(['ID', 'Ref', 'n(SMILES)', 'p(SMILES)'], axis=1)
        df2 = df2[df2[['n(CDK Fingerprint)', 'p(CDK Fingerprint)', 'n(RDKit Fingerprint)', 'p(RDKit Fingerprint)', 'n(Morgan Fingerprint)', 'p(Morgan Fingerprint)']].apply(lambda x: x.map(is_binary)).all(axis=1)]
        df2['CDK_fp']    = df2['n(CDK Fingerprint)'] + df2['p(CDK Fingerprint)']
        df2['RDKit_fp']  = df2['n(RDKit Fingerprint)'] + df2['p(RDKit Fingerprint)']
        df2['Morgan_fp'] = df2['n(Morgan Fingerprint)'] + df2['p(Morgan Fingerprint)']
        df2 = df2[['CDK_fp', 'RDKit_fp', 'Morgan_fp', 'PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']].reset_index(drop=True)

        # Combine FA+NFA fingerprints
        df = pd.concat([df1, df2], ignore_index=True)[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)', 'Morgan_fp']]

        # Load compressed features
        df3 = pd.read_csv('combined_data_compression_autoencoder.csv', delimiter=',')

        # Merge dataframes based on the 'Morgan_fp' column
        merged_df = pd.merge(df, df3, on='Morgan_fp', how='inner')
        X = merged_df[[str(i) for i in range(32)]]
        y = merged_df[['PCE_ave(%)', 'Jsc(mA/cm2)', 'FF', 'Voc(V)']]
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        return X, y, X_train, X_test, y_train, y_test

# Choice of data among:
# 1) The compressed fingerprints, or
# 2) The other descriptors

# ========================================================
# Model Training and Evaluation Utilities
# ========================================================
# This module provides utilities for:
# 1. Training machine learning models (Random Forest, AdaBoost, etc.)
# 2. Evaluating model performance with multiple metrics
# 3. Plotting feature importances
# 4. Safely initializing models with cached or tuned hyperparameters
# ========================================================

# ----------------------------
# Evaluation Function
# ----------------------------
def evaluate_model(model, X_test, y_test):
    """
    Evaluate a trained regression model using multiple metrics.

    Parameters:
    - model: trained regression model
    - X_test: test set features
    - y_test: test set target values

    Returns:
    - rmse: Root Mean Squared Error
    - pearson_corr: Pearson correlation coefficient
    - r2: R^2 score
    - mae: Mean Absolute Error
    """

    # Generate predictions
    y_pred = model.predict(X_test)

    # Flatten predictions if shape is (n_samples, 1)
    if len(y_pred.shape) == 2 and y_pred.shape[1] == 1:
        y_pred = y_pred.ravel()
    if len(y_test.shape) == 2 and y_test.shape[1] == 1:
        y_test = y_test.values.ravel()

    # Compute evaluation metrics
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    pearson_corr, _ = pearsonr(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Print metrics
    print(f'Test RMSE: {rmse:.4f}')
    print(f"Test MAE: {mae:.4f}")
    print(f'Pearson Correlation Coefficient: {pearson_corr:.4f}')
    print(f"R^2 Score: {r2}")

    return rmse, pearson_corr, r2, mae

# ----------------------------
# Feature Importance Plotting
# ----------------------------
def plot_feature_importance(model, title_complement, feature_names, plot_figname=None):
    """
    Plot feature importances for tree-based models.

    Parameters:
    - model: trained model with feature_importances_ attribute
    - title_complement: string to add to plot title
    - feature_names: list of feature names
    - plot_figname: optional filename to save the figure
    """

    # Try to retrieve feature importances
    try:
        importances = model.feature_importances_
    except AttributeError:
        importances = model.get_feature_importance()

    # Sort features by importance
    indices = np.argsort(importances)[::-1]

    # Print ranked feature importances
    print("Feature Importances:")
    for i in range(len(feature_names)):
        print(f"{i + 1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}")

    plt.figure(figsize=(10, 6))
    sns.barplot(x=importances[indices], y=np.array(feature_names)[indices])
    plt.title(f"Feature Importances ({model.__class__.__name__}), {title_complement}")
    
    # Save plot if filename provided
    if plot_figname:
        plt.tight_layout()
        plt.savefig(plot_figname, dpi=300)
        print(f"Figure saved as: {plot_figname}")
    
    plt.show()

# ----------------------------
# Train and Evaluate Multiple Targets
# ----------------------------
def train_and_evaluate(train_func, X_train, y_train, X_test, y_test, model_name, dataset_name,
                       plot_feat_importance=False, feature_names=None, save_metrics_dict=None):
    """
    Train models for multiple target variables and optionally plot feature importances.

    Parameters:
    - train_func: function to train a single-target model
    - X_train, y_train: training data
    - X_test, y_test: test data
    - model_name: name of the model (used for plots & caching)
    - dataset_name: name of dataset (used for caching)
    - plot_feat_importance: whether to generate feature importance plots
    - feature_names: names of features for plotting
    - save_metrics_dict: optional dict to store evaluation metrics
    """

    models = {}
    targets = ['PCE_ave(%)', 'Jsc(mA/cm2)', 'Voc(V)', 'FF']

    for target in targets:
        filename_suffix = target.lower().split('(')[0].strip()
        print(f"\nTraining on: {target}")

        # Train model for the target
        try:
            model = train_func(X_train, y_train[target], dataset_name=dataset_name)
        except TypeError:
            # Fallback for old functions without dataset_name
            model = train_func(X_train, y_train[target])

        # Evaluate model
        rmse, pearson_corr, r2, mae = evaluate_model(model, X_test, y_test[target])
        if save_metrics_dict is not None:
            save_metrics_dict[(model_name, dataset_name, target)] = (rmse, pearson_corr, r2, mae)

        # Feature importance plot
        if plot_feat_importance:
            model_nm = { "Random Forest": "RF",
                         "Ada Boost": "Ada",
                         "Cat Boost": "Cat",
                         "Cat Boost Optuna": "Cat",
                         "XGBoost": "XGB",
                         "LightGBM": "LGBM" }
            plot_figname = f"{model_nm[model_name]}_{filename_suffix}_{dataset_name}.png"
            if feature_names is not None:
                plot_feature_importance(model, title_complement=target.split('(')[0],
                                        feature_names=feature_names, plot_figname=plot_figname)
            else:
                plot_feature_importance(model, title_complement=target.split('(')[0], plot_figname=plot_figname)

        # Retrain on full dataset (train + test) using cached best params
        X_full = pd.concat([X_train, X_test], axis=0)
        y_full = pd.concat([y_train[target], y_test[target]], axis=0)
        try:
            final_model = train_func(X_full, y_full, dataset_name=dataset_name)
        except TypeError:
            final_model = train_func(X_full, y_full)

        models[target] = final_model

    return models

# ----------------------------
# Safe Model Initialization
# ----------------------------
import inspect

def safe_init(model_class, params):
    """
    Safely initialize a model, removing incompatible parameters.

    Parameters:
    - model_class: class of the model (e.g., RandomForestRegressor)
    - params: dictionary of parameters to use

    Returns:
    - instance of model_class with compatible parameters
    """
    clean_params = dict(params)  # copy to avoid mutating cache
    name = model_class.__name__.lower()

     # CatBoost uses "random_seed" instead of "random_state"
    if "catboost" in name:
        clean_params.pop("random_state", None)
        clean_params.setdefault("random_seed", 42)
    # Others (sklearn, XGBoost, LightGBM) --> keep "random_state"
    else:
        clean_params.pop("random_seed", None)
        clean_params.setdefault("random_state", 42)

    # Filter out any parameters not accepted by model __init__
    valid_params = inspect.signature(model_class.__init__).parameters
    clean_params = {k: v for k, v in clean_params.items() if k in valid_params}

    return model_class(**clean_params)


# ========================================================
# Random Forest Training
# ========================================================
from sklearn.ensemble import RandomForestRegressor

# Cache best params per dataset
_rf_best_params_cache = {}

def train_rf(X_train, y_train, dataset_name="default"):
    """
    Train a Random Forest model using Bayesian hyperparameter optimization.
    If parameters are cached, uses them directly.

    Returns:
    - trained RandomForestRegressor instance
    """

    global _rf_best_params_cache

    # If already tuned for this dataset --> skip optimization
    if dataset_name in _rf_best_params_cache:
        best_params = _rf_best_params_cache[dataset_name]
        best_rf = safe_init(RandomForestRegressor, best_params, random_state=42)
        best_rf.fit(X_train, y_train)
        return best_rf

    # Define search space using skopt types
    param_space = {
        'n_estimators': Integer(100, 1000),
        'max_depth': Integer(5, 50),
        'max_features': Categorical(['log2', 'sqrt']),
        'min_samples_split': Integer(2, 10),
        'min_samples_leaf': Integer(1, 10),
        'bootstrap': Categorical([True, False])
    }

    # Bayesian search
    opt = BayesSearchCV(
        estimator=RandomForestRegressor(random_state=42),
        search_spaces=param_space,
        n_iter=100,
        cv=3,
        n_jobs=-1,
        random_state=42
    )

    opt.fit(X_train, y_train)
    print(f"Best hyperparameters (Random Forest) for {dataset_name}: {opt.best_params_}")

    # Store in cache including random_state
    _rf_best_params_cache[dataset_name] = {**opt.best_params_, "random_state": 42}

    return opt.best_estimator_

# ========================================================
# AdaBoost Training
# ========================================================
from sklearn.ensemble import AdaBoostRegressor

# Cache best params per dataset
_ada_best_params_cache = {}

def train_ada(X_train, y_train, dataset_name="default"):
    """
    Train an AdaBoost model using Bayesian hyperparameter optimization.
    If parameters are cached, uses them directly.

    Returns:
    - trained AdaBoostRegressor instance
    """

    global _ada_best_params_cache

    # If already tuned for this dataset â†’ skip optimization
    if dataset_name in _ada_best_params_cache:
        best_params = _ada_best_params_cache[dataset_name]
        best_ada = safe_init(AdaBoostRegressor, best_params, random_state=42)
        best_ada.fit(X_train, y_train)
        return best_ada

    # Define search space
    param_space_ada = {
        'n_estimators': Integer(50, 500),
        'learning_rate': Real(0.01, 1.0, prior='log-uniform'),
    }

    # Bayesian search
    opt_ada = BayesSearchCV(
        estimator=AdaBoostRegressor(random_state=42),
        search_spaces=param_space_ada,
        n_iter=100,
        cv=3,
        n_jobs=-1,
        random_state=42
    )

    opt_ada.fit(X_train, y_train)
    print(f"Best hyperparameters (AdaBoost) for {dataset_name}: {opt_ada.best_params_}")

    # Cache best params
    _ada_best_params_cache[dataset_name] = {**opt_ada.best_params_, "random_state": 42}

    return opt_ada.best_estimator_

# ========================================================
# CatBoost Training
# ========================================================
from catboost import CatBoostRegressor, cv, Pool
import torch
import shutil  # For cleaning previous CatBoost directories

# Optuna is preferred for CatBoost hyperparameter optimization:
# - Supports conditional parameters
# - Can leverage GPU if available

# Cache best params for each dataset
_cat_best_params_cache = {}

def train_cat(X_train, y_train, dataset_name="default"):
    """
    Train a CatBoost model using Optuna for hyperparameter optimization.
    Uses cached parameters if available.

    Returns:
    - trained CatBoostRegressor instance
    """

    global _cat_best_params_cache

    # Reuse cached best parameters if available
    if dataset_name in _cat_best_params_cache:
        best_params = dict(_cat_best_params_cache[dataset_name])
        best_params["verbose"] = 0
        best_params["random_state"] = 42
        best_cat = safe_init(CatBoostRegressor, best_params)
        best_cat.fit(X_train, y_train)
        return best_cat

    # Define Optuna objective
    def objective(trial):
        params = {
            'iterations': trial.suggest_int('iterations', 100, 500),
            'depth': trial.suggest_int('depth', 1, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0, log=True),
            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-5, 100, log=True),
            'boosting_type': trial.suggest_categorical('boosting_type', ['Ordered', 'Plain']),
            'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli']),
            'random_seed': 42,
            'verbose': 0,
            'task_type': "GPU" if torch.cuda.is_available() else 'CPU',
            'loss_function': 'RMSE'
        }

        if params['bootstrap_type'] == 'Bernoulli':
            params['subsample'] = trial.suggest_float('subsample', 0.9, 1.0)
        elif params['bootstrap_type'] == 'Bayesian':
            params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0.0, 1.0)

        cv_data = Pool(X_train, y_train)
        cv_results = cv(cv_data, params, fold_count=3, logging_level='Silent')
        mean_cv_loss = cv_results['test-RMSE-mean'].values[-1]

        return mean_cv_loss

    # Clean previous CatBoost artifacts
    train_dir = 'catboost_info'
    if os.path.exists(train_dir):
        shutil.rmtree(train_dir)

    # Run Optuna optimization
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=80)

    # Store best params
    best_params = dict(study.best_params)
    best_params['task_type'] = 'GPU' if torch.cuda.is_available() else 'CPU'
    best_params['loss_function'] = 'RMSE'
    best_params['random_seed'] = 42
    best_params['verbose'] = 0

    _cat_best_params_cache[dataset_name] = best_params
    print(f"Best hyperparameters (CatBoost) for {dataset_name}: {best_params}")

    # Train best model
    final_params = dict(best_params)
    final_params["verbose"] = 0
    final_params["random_state"] = 42
    best_model = safe_init(CatBoostRegressor, final_params)
    best_model.fit(X_train, y_train)

    return best_model


# ========================================================
# XGBoost Training
# ========================================================
from xgboost import XGBRegressor

# Cache best params per dataset
_xgb_best_params_cache = {}

def train_xgb(X_train, y_train, dataset_name="default"):
    """
    Train an XGBoost model using Optuna for hyperparameter optimization.
    Returns trained XGBRegressor instance.
    """

    global _xgb_best_params_cache

    # Use cached parameters if available
    if dataset_name in _xgb_best_params_cache:
        best_params = _xgb_best_params_cache[dataset_name]
        best_xgb = safe_init(
            XGBRegressor,
            best_params,
            random_state=42,
            verbosity=0,
            n_jobs=-1
        )
        best_xgb.fit(X_train, y_train)
        return best_xgb

    # Optuna objective
    def objective(trial):
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 100, 500),
            "max_depth": trial.suggest_int("max_depth", 3, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 1.0, log=True),
            "subsample": trial.suggest_float("subsample", 0.5, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
            "gamma": trial.suggest_float("gamma", 1e-8, 1.0, log=True),
            "random_state": 42,
            "verbosity": 0,
            "n_jobs": -1
        }

        model = XGBRegressor(**params)
        score = cross_val_score(
            model, X_train, y_train, cv=3,
            scoring="neg_mean_squared_error", n_jobs=-1
        )
        return np.mean(score)

    # Run optimization
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=100, n_jobs=1)

    # Cache best parameters
    _xgb_best_params_cache[dataset_name] = {
        **study.best_params,
        "random_state": 42,
        "verbosity": 0,
        "n_jobs": -1
    }

    print(f"Best hyperparameters (XGBoost) for {dataset_name}: {_xgb_best_params_cache[dataset_name]}")

    # Train best model safely
    best_xgb = safe_init(
        XGBRegressor,
        _xgb_best_params_cache[dataset_name],
        random_state=42,
        verbosity=0,
        n_jobs=-1
    )
    best_xgb.fit(X_train, y_train)

    return best_xgb

# ========================================================
# LightGBM Training
# ========================================================
import lightgbm as lgb
from packaging import version

_lgbm_best_params_cache = {}

def train_lgb(X_train, y_train, dataset_name="default"):
    """
    Train a LightGBM model using Optuna optimization with early stopping.
    Supports both pre-4.0 and >=4.0 versions of LightGBM.
    Returns trained LGBMRegressor instance.
    """

    global _lgbm_best_params_cache

    # Determine LightGBM version for callback compatibility
    lgb_version = version.parse(lgb.__version__)
    use_callbacks = lgb_version >= version.parse("4.0.0")

    # Reuse tuned parameters if cached
    if dataset_name in _lgbm_best_params_cache:
        best_params = dict(_lgbm_best_params_cache[dataset_name])
        best_lgb = safe_init(lgb.LGBMRegressor, best_params)
        
        fit_kwargs = {
            "X": X_train,
            "y": y_train
        }

        if use_callbacks:
            fit_kwargs["eval_set"] = [(X_train, y_train)]
            fit_kwargs["eval_metric"] = "rmse"
            fit_kwargs["callbacks"] = [lgb.early_stopping(30), lgb.log_evaluation(-1)]
        else:
            fit_kwargs["eval_set"] = [(X_train, y_train)]
            fit_kwargs["eval_metric"] = "rmse"
            fit_kwargs["early_stopping_rounds"] = 30
            fit_kwargs["verbose"] = -1

        best_lgb.fit(**fit_kwargs)
        return best_lgb

    # Prepare Dataset for CV
    lgb_train = lgb.Dataset(X_train, y_train)

    # Optuna objective
    def objective(trial):
        params = {
            "objective": "regression",
            "metric": "rmse",
            "boosting_type": "gbdt",
            "n_estimators": 500,
            "max_depth": trial.suggest_int("max_depth", 3, 10),
            "learning_rate": trial.suggest_float("learning_rate", 0.02, 0.3, log=True),
            "num_leaves": trial.suggest_int("num_leaves", 31, 200),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "random_state": 42,
            "n_jobs": -1,
            "verbose": -1
        }

        callbacks = [lgb.log_evaluation(-1)]
        if use_callbacks:
            callbacks.append(lgb.early_stopping(30))

        cv_results = lgb.cv(
            params,
            lgb_train,
            folds=None,
            nfold=3,
            stratified=False,
            seed=42,
            callbacks=callbacks
        )

        # Extract RMSE metric in a version-agnostic way
        metric_key = next(
            (k for k in cv_results.keys() if "mean" in k and ("rmse" in k or "l2_root" in k)),
            None
        )
        if metric_key is None:
            raise ValueError(f"No RMSE metric found in cv_results. Keys: {list(cv_results.keys())}")

        return -min(cv_results[metric_key])

    # Run Optuna study
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=100, n_jobs=1)

    # Store best parameters
    best_params = dict(study.best_params)
    best_params.update({
        "objective": "regression",
        "metric": "rmse",
        "random_state": 42,
        "n_jobs": -1,
        "verbose": -1
    })
    _lgbm_best_params_cache[dataset_name] = best_params
    print(f"Best hyperparameters (LightGBM) for {dataset_name}: {best_params}")

    # Train final model
    best_lgb = safe_init(lgb.LGBMRegressor, best_params)
    fit_kwargs = {
        "X": X_train,
        "y": y_train
    }
    if use_callbacks:
        fit_kwargs["eval_set"] = [(X_train, y_train)]
        fit_kwargs["eval_metric"] = "rmse"
        fit_kwargs["callbacks"] = [lgb.early_stopping(30), lgb.log_evaluation(-1)]
    else:
        fit_kwargs["eval_set"] = [(X_train, y_train)]
        fit_kwargs["eval_metric"] = "rmse"
        fit_kwargs["early_stopping_rounds"] = 30
        fit_kwargs["verbose"] = -1

    best_lgb.fit(**fit_kwargs)
    return best_lgb

# ========================================================
# Training Execution for Descriptors and Compressed Fingerprints
# ========================================================
# This section handles:
# 1. Training models on "other descriptors" (non-fingerprint features)
# 2. Training models on compressed fingerprints
# 3. Saving trained models
# 4. Logging errors during training or saving
# ========================================================


# ----------------------------
# Training on "other descriptors" (not fingerprints)
# ----------------------------
import logging

# Configure logging for training errors
logging.basicConfig(filename='model_training_errors.log', level=logging.ERROR)

data = 'others'
X, y, X_1, y_1, X_2, y_2, X_train, X_test, y_train, y_test, X_train_1, X_test_1, y_train_1, y_test_1, X_train_2, X_test_2, y_train_2, y_test_2 = load_data(data)

datasets = {
    "FA+NFA": (X_train, y_train, X_test, y_test, X.columns),
    "FA":     (X_train_1, y_train_1, X_test_1, y_test_1, X_1.columns),
    "NFA":    (X_train_2, y_train_2, X_test_2, y_test_2, X.columns)
}

model_funcs = {
    "Random Forest": train_rf,
    "Ada Boost": train_ada,
    "Cat Boost": train_cat,
    "XGBoost": train_xgb,
    "LightGBM": train_lgb
}

# ==== Execution ====
# Iterate through models and datasets
for model_name, train_func in model_funcs.items():
    print(f"\n===== {model_name} =====")
    for dataset_name, (X_tr, y_tr, X_te, y_te, feat_names) in datasets.items():
        print(f"\n--- Results {dataset_name} ---")
        try:
            # Train and evaluate models for multiple targets
            models = train_and_evaluate(
                train_func, X_tr, y_tr, X_te, y_te,
                feature_names=feat_names, plot_feat_importance=True,
                model_name=model_name, dataset_name=dataset_name, save_metrics_dict=metrics_others
            )

            # Save models
            for target, model in models.items():
                # Clean target string for filename
                filename = f"{model_name.replace(' ', '_')}_{dataset_name}_{target.replace('_ave(%)','').replace('(mA/cm2)','').replace('(V)','')}"
                try:
                    # Train and evaluate models for multiple targets
                    if model_name.startswith("Cat Boost"):
                        model.save_model(filename + ".cbm")
                    elif model_name == "XGBoost":
                        model.save_model(filename + ".xgb")
                    elif model_name == "LightGBM":
                        # Handle sklearn API
                        if hasattr(model, "booster_") and hasattr(model.booster_, "save_model"):
                            model.booster_.save_model(filename + ".lgb")
                        # Handle native Booster API
                        elif hasattr(model, "save_model"):
                            model.save_model(filename + ".lgb")
                        else:
                            joblib.dump(model, filename + ".joblib")
                    else:
                        joblib.dump(model, filename + ".joblib")
                    print(f"Saved model: {filename}")
                except Exception as save_err:
                    print(f"Error saving model {filename}: {save_err}")

        except Exception as train_err:
            # Log any errors during training
            print(f"Error training {model_name} on {dataset_name}: {train_err}")
            logging.error(f"Error training {model_name} on {dataset_name}: {train_err}")

# Save metrics for LaTeX reporting
save_metrics_latex(metrics_dict=metrics_others, filename = "metrics_other.txt", dataset_label="Metrics for float descriptors")

# ----------------------------
# Training on compressed fingerprints
# ----------------------------

data = 'compressed_fingerprints'
X, y, X_train, X_test, y_train, y_test = load_data(data)

# Single-dataset dictionary for consistency with previous loop
datasets_compressed = {
    "compressed_fp": (X_train, y_train, X_test, y_test, None)  # No feature names for compressed fingerprints
}

# === Execution ===
# Iterate through models and the compressed fingerprint dataset
for model_name, train_func in model_funcs.items():
    print(f"\n===== {model_name} =====")
    for dataset_name, (X_tr, y_tr, X_te, y_te, feat_names) in datasets_compressed.items():
        print(f"\n--- Results: {dataset_name} ---")
        try:
            models = train_and_evaluate(
                train_func, X_tr, y_tr, X_te, y_te,
                feature_names=feat_names, model_name=model_name, dataset_name=dataset_name,
                save_metrics_dict=metrics_compressed_fp
            )
            # Save models
            for target, model in models.items():
                target_clean = target.replace('_ave(%)','').replace('(mA/cm2)','').replace('(V)','')
                filename = f"{model_name.replace(' ', '_')}_{dataset_name}_{target_clean}"
                try:
                    if model_name.startswith("Cat Boost"):
                        model.save_model(filename + ".cbm")
                    elif model_name == "XGBoost":
                        model.save_model(filename + ".xgb")
                    elif model_name == "LightGBM":
                        # Handle sklearn API
                        if hasattr(model, "booster_") and hasattr(model.booster_, "save_model"):
                            model.booster_.save_model(filename + ".lgb")
                        # Handle native Booster API
                        elif hasattr(model, "save_model"):
                            model.save_model(filename + ".lgb")
                        else:
                            joblib.dump(model, filename + ".joblib")
                    else:
                        joblib.dump(model, filename + ".joblib")
                    print(f"Saved model: {filename}")
                except Exception as save_err:
                    print(f"Error saving model {filename}: {save_err}")

        except Exception as train_err:
            print(f"Error training {model_name} on {dataset_name}: {train_err}")
            logging.error(f"Error training {model_name} on {dataset_name}: {train_err}")

# Save metrics for LaTeX reporting
save_metrics_latex(metrics_dict=metrics_compressed_fp, filename = "metrics_compressed_fp.txt", dataset_label="Metrics for compressed fingerprints")
